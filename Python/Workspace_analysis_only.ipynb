{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopkins-UCSD vocabulary mapping collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document pulls together working code that is specific to the OMOP mapping project. In particular, this notebook is the taken from the master notebook, and represents functionality for the mapping analysis (after mapping has been completed).\n",
    "\n",
    "Most of the working functions have been moved to `Resources/custom_funcs.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code setup (run every time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(1, 'Resources')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sssom.io\n",
    "import yaml\n",
    "import plotly.graph_objects as go\n",
    "from datamanagement import valuedef_update, get_eldef, get_valdef\n",
    "from custom_funcs import *\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling core definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eldef = get_eldef(); assert df_eldef.CUI.is_unique\n",
    "df_valdef = get_valdef(); assert df_valdef.ID.is_unique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What elements are included? (run as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exam area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elcount_byexamarea = pd.DataFrame(df_eldef.examArea.value_counts())\\\n",
    "    .reset_index().rename(columns={\"index\":\"examArea\", \"examArea\":\"Element Counts\"})\n",
    "\n",
    "df_elcount_byexamarea.to_csv(\"Exports/DefinitionCounts/ElementsByExamArea.csv\", index=False)\n",
    "df_elcount_byexamarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepopulated values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By exam area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valcount_byexamarea = pd.DataFrame(df_valdef.merge(df_eldef, on=\"CUI\").examArea.value_counts())\\\n",
    "    .reset_index().rename(columns={\"index\":\"examArea\", \"examArea\":\"Prepopulated Option Counts\"})\n",
    "\n",
    "df_valcount_byexamarea.to_csv(\"Exports/DefinitionCounts/PrepopulatedOptionsByExamArea.csv\", index=False)\n",
    "df_valcount_byexamarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By data element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valcount_bydataelement = pd.DataFrame(df_valdef.CUI.value_counts()).rename(columns={\"CUI\":\"Prepopulated Option Counts\"}).merge(df_eldef, left_index=True, right_on=\"CUI\")\\\n",
    "    [[\"CUI\", \"examArea\", \"dataElement\", \"Prepopulated Option Counts\"]]\\\n",
    "        .drop(columns=[\"CUI\"])\n",
    "\n",
    "df_valcount_bydataelement.to_csv(\"Exports/DefinitionCounts/PrepopulatedOptionsByElement.csv\", index=False)\n",
    "df_valcount_bydataelement.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse mappings (run as needed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output directory, if doesn't already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_out = True\n",
    "outdir = create_outdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping document paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_el_sb_path = \"Resources/Mappings/SB_ElementMapping.xlsx\"\n",
    "df_el_cc_path = \"Resources/Mappings/CC_ElementMapping.xlsx\"\n",
    "df_el_consensus_path = \"Resources/Mappings/CONS_ElementMapping.xlsx\"\n",
    "df_val_sb_path = \"Resources/Mappings/SB_ValueMapping.xlsx\"\n",
    "df_val_cc_path = \"Resources/Mappings/CC_ValueMapping.xlsx\"\n",
    "df_val_consensus_path = \"Resources/Mappings/CONS_ValueMapping.xlsx\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element mappings\n",
    "df_el_sb = pd.read_excel(df_el_sb_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_el_cc = pd.read_excel(df_el_cc_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_el_consensus = pd.read_excel(df_el_consensus_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", \"comment\"]]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\"})\n",
    "\n",
    "# Value mappings\n",
    "df_val_sb = pd.read_excel(df_val_sb_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_val_cc = pd.read_excel(df_val_cc_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_val_consensus = pd.read_excel(df_val_consensus_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", \"comment\"]]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\"})\n",
    "\n",
    "# Assert that they're all the same shape\n",
    "assert df_el_consensus.shape == df_el_sb.shape == df_el_cc.shape\n",
    "assert df_val_consensus.shape == df_val_sb.shape == df_val_cc.shape\n",
    "\n",
    "# The following section is to filter out exam areas that were excluded from the study\n",
    "apply_filter = True\n",
    "\n",
    "if apply_filter:\n",
    "    (df_el_sb, df_el_cc, df_el_consensus) = custom_filter((df_el_sb, df_el_cc, df_el_consensus), 'element')\n",
    "    (df_val_sb, df_val_cc, df_val_consensus) = custom_filter((df_val_sb, df_val_cc, df_val_consensus), 'value')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create get breakdown analysis values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_version = 2\n",
    "dict_analyse = {\n",
    "    \"SB elements\":analyze_mapping(df_el_sb, analysis_version=analysis_version),\n",
    "    \"CC elements\":analyze_mapping(df_el_cc, analysis_version=analysis_version),\n",
    "    \"SB values\":analyze_mapping(df_val_sb, analysis_version=analysis_version),\n",
    "    \"CC values\":analyze_mapping(df_val_cc, analysis_version=analysis_version),\n",
    "    \"CONS elements\":analyze_mapping(df_el_consensus, analysis_version=analysis_version),\n",
    "    \"CONS values\":analyze_mapping(df_val_consensus, analysis_version=analysis_version)\n",
    "}\n",
    "\n",
    "if save_out:\n",
    "    with open(outdir + \"/Analysis/filtered_values.json\", 'w') as outfile:\n",
    "        json.dump(dict_analyse, outfile, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual_maps = [\"SB elements\", \"CC elements\", \"SB values\", \"CC values\"]\n",
    "# consensus_maps = [\"CONS elements\", \"CONS values\"]\n",
    "# working_list = consensus_maps   #use this to select what's viewed\n",
    "\n",
    "# for label_working in working_list:\n",
    "#     dict_working = dict_analyse[label_working]\n",
    "#     fig, ax = plt.subplots(ncols=3, figsize=[20,4])\n",
    "\n",
    "#     labels, values = zip(*dict_working[\"equivalence\"].items())\n",
    "#     ax[0].pie(values, labels=labels);\n",
    "#     ax[0].set_title(\"Equivalence\");\n",
    "\n",
    "#     labels, values = zip(*dict_working[\"unmapped\"].items())\n",
    "#     ax[1].pie(values, labels=labels);\n",
    "#     ax[1].set_title(\"Flags for UNMAPPED\");\n",
    "\n",
    "#     labels, values = zip(*dict_working[\"wider\"].items())\n",
    "#     ax[2].pie(values, labels=labels);\n",
    "#     ax[2].set_title(\"Flags for WIDER\");\n",
    "\n",
    "#     if save_out: plt.savefig(outdir + \"/Analysis/\" + label_working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sankey diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_list = [\"SB elements\", \"CC elements\", \"SB values\", \"CC values\", \"CONS elements\", \"CONS values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighten_color(color, amount=0.5):\n",
    "    \"\"\"\n",
    "    Lightens the given color by multiplying (1-luminosity) by the given amount.\n",
    "    Input can be matplotlib color string, hex string, or RGB tuple.\n",
    "\n",
    "    Examples:\n",
    "    >> lighten_color('g', 0.3)\n",
    "    >> lighten_color('#F034A3', 0.6)\n",
    "    >> lighten_color((.3,.55,.1), 0.5)\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    out = colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])\n",
    "    return \"rgb\" + str(tuple(map(lambda x: round(x*255), out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_map = {'BASE': \"#808080\",\n",
    "    'EQUAL': \"#47dcf5\",#            Stoled from INDIRECT \"#ffb47b\",\n",
    "    'WIDER': \"#33d6ab\",\n",
    "    'NARROWER': \"#f27762\",\n",
    "    'UNMATCHED': \"#d56f8c\",\n",
    "    'VALSMAPPED': \"#ffacff\",\n",
    "    'NOMATCH': \"#d56f8c\",\n",
    "    'INDIRECT': \"#47dcf5\",\n",
    "    'SUBFIELD': \"#000000\",\n",
    "    'LATERALITY': \"#c5ed99\",\n",
    "    'CONCEPTMISSING': \"#bc82fb\",\n",
    "    'CONCEPTMISSING&LATERALITY': \"#fed575\",\n",
    "    'OTHER': \"#ffacff\" # give it the same colour as VALSMAPPED \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colours = [\"#808080\", \"#ffb47b\", \"#33d6ab\", \"#f27762\", \"#d56f8c\", \"#000000\", \"#d56f8c\", \"#c5ed99\", \"#47dcf5\", \"#ffacff\", \"#fed575\", \"#bc82fb\"]\n",
    "# default_colours_b = [\"#808080\", \"#d56f8c\",\"#33d6ab\",\"#f27762\",\"#ffb47b\",\"#bc82fb\",\"#c5ed99\",\"#fed575\",\"#ffacff\",\"#47dcf5\",\"#d56f8c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __labelhierarchycategorize(label):\n",
    "    if label in [\"EQUAL\", \"WIDER\", \"NARROWER\", \"UNMATCHED\"]:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_sankey_values_from_dictanalyse(mdict):\n",
    "    dictkeymap = {\n",
    "        \"equivalence\": \"BASE\",\n",
    "        \"unmapped\":\"UNMATCHED\",\n",
    "        \"wider\":\"WIDER\"\n",
    "    }\n",
    "\n",
    "    namemap_counter = 0\n",
    "    namemap_dict = {}\n",
    "\n",
    "    source = []\n",
    "    target = []\n",
    "    value = []\n",
    "    labels = []\n",
    "    xvalues = []\n",
    "    yvalues = []\n",
    "    node_colours = []\n",
    "    link_colors = []\n",
    "\n",
    "    iter = 0\n",
    "\n",
    "    for dictkey in mdict.keys():\n",
    "        if dictkeymap[dictkey] not in namemap_dict.keys():\n",
    "            namemap_dict[dictkeymap[dictkey]] = namemap_counter\n",
    "            namemap_counter += 1\n",
    "            if dictkeymap[dictkey] == \"BASE\":\n",
    "                labels.append(\"\")#\"COUNT: \" + str(sum(mdict[dictkey].values())))\n",
    "                xvalues.append(0)\n",
    "                yvalues.append(0.0)\n",
    "                node_colours.append(colour_map[\"BASE\"])\n",
    "        \n",
    "        for label in mdict[dictkey]:\n",
    "            if label not in namemap_dict.keys():\n",
    "                namemap_dict[label] = namemap_counter\n",
    "                namemap_counter += 1\n",
    "                labels.append(label + \": \" + str(mdict[dictkey][label]))\n",
    "                xvalues.append(__labelhierarchycategorize(label))\n",
    "                yvalues.append(0.2 - iter)\n",
    "                node_colours.append(colour_map[label])\n",
    "                iter += 0.02\n",
    "\n",
    "            source.append(namemap_dict[dictkeymap[dictkey]])\n",
    "            target.append(namemap_dict[label])\n",
    "            value.append(mdict[dictkey][label])\n",
    "            link_colors.append(lighten_color(colour_map[label]))\n",
    "            \n",
    "    return namemap_dict, source, target, value, labels, xvalues, yvalues, node_colours, link_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namemap_dict, source, target, value, labels, xvalues, yvalues, node_colours, link_colors = get_sankey_values_from_dictanalyse(\\\n",
    "    combine_analyse(dict_analyse[\"CONS elements\"], dict_analyse[\"CONS values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement = \"snap\",\n",
    "    valueformat = \".0f\",\n",
    "    valuesuffix = \"TWh\",\n",
    "    # Define nodes\n",
    "    meta = dict(\n",
    "      textposition=\"top center\"\n",
    "    ),\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 15,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = list(map((lambda lab: \"\"), labels)),\n",
    "      x = xvalues,\n",
    "      y = yvalues,\n",
    "      color = node_colours\n",
    "    ),\n",
    "    # Add links\n",
    "    link = dict(\n",
    "      source =  source,\n",
    "      target =  target,\n",
    "      value =  value,\n",
    "      color = link_colors\n",
    "))])\n",
    "\n",
    "fig.update_layout(title_text=\"Analysis Sankey Diagram\",\n",
    "    font_size=15,\n",
    "    width = 800,\n",
    "    height=600)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create expanded flags spreadsheets to allow Excel analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_out:\n",
    "    append_sourceel_origindex(append_vocabulary_id(append_sourceel_names(append_concept_names(expand_flags(df_el_sb)))))\\\n",
    "        [[\"orig_index\", \"examArea\", \"dataElement\", \"concept_name\", \"equivalence\", \"sourceCode\", \"conceptId\", \"vocabulary_id\", \"NOMATCH\", \"VALSMAPPED\", \"INDIRECT\", \"LATERALITY\", \"CONCEPTMISSING\", \"SUBFIELD\"]]\\\n",
    "            .to_excel(outdir + \"/Analysis/FlagsExpanded/elements_sb.xlsx\", index=False)\n",
    "    append_sourceel_origindex(append_vocabulary_id(append_sourceel_names(append_concept_names(expand_flags(df_el_cc)))))\\\n",
    "        [[\"orig_index\", \"examArea\", \"dataElement\", \"concept_name\", \"equivalence\", \"sourceCode\", \"conceptId\", \"vocabulary_id\", \"NOMATCH\", \"VALSMAPPED\", \"INDIRECT\", \"LATERALITY\", \"CONCEPTMISSING\", \"SUBFIELD\"]]\\\n",
    "            .to_excel(outdir + \"/Analysis/FlagsExpanded/elements_cc.xlsx\", index=False)\n",
    "    append_vocabulary_id(append_sourceval_names(append_concept_names(expand_flags(df_val_sb))))\\\n",
    "        [[\"examArea\", \"dataElement\", \"value\", \"concept_name\", \"equivalence\", \"sourceCode\", \"conceptId\", \"vocabulary_id\", \"NOMATCH\", \"LATERALITY\", \"CONCEPTMISSING\", \"SUBFIELD\"]]\\\n",
    "            .to_excel(outdir + \"/Analysis/FlagsExpanded/values_sb.xlsx\", index=False)\n",
    "    append_vocabulary_id(append_sourceval_names(append_concept_names(expand_flags(df_val_cc))))\\\n",
    "        [[\"examArea\", \"dataElement\", \"value\", \"concept_name\", \"equivalence\", \"sourceCode\", \"conceptId\", \"vocabulary_id\", \"NOMATCH\", \"LATERALITY\", \"CONCEPTMISSING\", \"SUBFIELD\"]]\\\n",
    "            .to_excel(outdir + \"/Analysis/FlagsExpanded/values_cc.xlsx\", index=False)\n",
    "    append_sourceel_origindex(append_vocabulary_id(append_sourceel_names(append_concept_names(expand_flags(df_el_consensus)))))\\\n",
    "        [[\"orig_index\", \"examArea\", \"dataElement\", \"concept_name\", \"equivalence\", \"sourceCode\", \"conceptId\", \"vocabulary_id\", \"NOMATCH\", \"VALSMAPPED\", \"INDIRECT\", \"LATERALITY\", \"CONCEPTMISSING\", \"SUBFIELD\"]]\\\n",
    "            .to_excel(outdir + \"/Analysis/FlagsExpanded/elements_consensus.xlsx\", index=False)\n",
    "    append_vocabulary_id(append_sourceval_names(append_concept_names(expand_flags(df_val_consensus))))\\\n",
    "        [[\"examArea\", \"dataElement\", \"value\", \"concept_name\", \"equivalence\", \"sourceCode\", \"conceptId\", \"vocabulary_id\", \"NOMATCH\", \"LATERALITY\", \"CONCEPTMISSING\", \"SUBFIELD\"]]\\\n",
    "            .to_excel(outdir + \"/Analysis/FlagsExpanded/values_consensus.xlsx\", index=False)\n",
    "\n",
    "# Verify above expansion\n",
    "verify_list = [\"elements_consensus\", \"values_consensus\"]\n",
    "for verify_string in verify_list:\n",
    "    df_temp = pd.read_excel((outdir + \"/Analysis/FlagsExpanded/\" + verify_string + \".xlsx\"))\n",
    "\n",
    "    #Assert mutually exclusive \"UNMATCHED\" flags\n",
    "    unmatched_flaglist = list({\"NOMATCH\", \"VALSMAPPED\", \"INDIRECT\", \"SUBFIELD\"} & set(df_temp.columns))\n",
    "    assert (df_temp.loc[df_temp.equivalence == \"UNMATCHED\"][unmatched_flaglist].sum(axis=1) == 1).all()\n",
    "\n",
    "    #Assert presence of \"WIDER\" flags\n",
    "    wider_flaglist = list({\"LATERALITY\", \"CONCEPTMISSING\"} & set(df_temp.columns))\n",
    "    assert (df_temp.loc[df_temp.equivalence == \"WIDER\"][wider_flaglist].sum(axis=1) > 0).all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate specific subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analyse = df_el_consensus\n",
    "equiv = \"WIDER\"\n",
    "flag = \"CONCEPTMISSING\"\n",
    "\n",
    "df_sliced = append_vocabulary_id(append_sourceel_names(append_concept_names(rows_by_equiv_and_flag(df_analyse, flag, equiv))))\\\n",
    "    [[\"sourceCode\", \"examArea\", \"dataElement\", \"equivalence\", \"conceptId\", \"concept_name\", \"vocabulary_id\"]]\n",
    "\n",
    "[print(index + \":\\t\" + str(value)) for index,value in df_sliced.vocabulary_id.value_counts().iteritems()]\n",
    "print(\"Total:\\t%d\" % df_sliced.shape[0])\n",
    "df_sliced.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown by source vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_vocabulary_id(append_sourceel_names(append_concept_names(rows_by_equiv_and_flag(df_analyse, flag, equiv))))\\\n",
    "    .vocabulary_id.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing screens (run as needed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create output directory, if doesn't already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_out = True\n",
    "outdir = create_outdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import mappings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping document paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_el_wh_path = \"Resources/Mappings/WH_ElementMapping.xlsx\"\n",
    "df_el_sb_path = \"Resources/Mappings/SB_ElementMapping.xlsx\"\n",
    "df_el_cc_path = \"Resources/Mappings/CC_ElementMapping.xlsx\"\n",
    "df_el_consensus_path = \"Resources/Mappings/CONS_ElementMapping.xlsx\"\n",
    "df_val_wh_path = \"Resources/Mappings/WH_ValueMapping.xlsx\"\n",
    "df_val_sb_path = \"Resources/Mappings/SB_ValueMapping.xlsx\"\n",
    "df_val_cc_path = \"Resources/Mappings/CC_ValueMapping.xlsx\"\n",
    "df_val_consensus_path = \"Resources/Mappings/CONS_ValueMapping.xlsx\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import mappings as dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element mappings\n",
    "df_el_wh = pd.read_excel(df_el_wh_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_el_sb = pd.read_excel(df_el_sb_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_el_cc = pd.read_excel(df_el_cc_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_el_consensus = pd.read_excel(df_el_consensus_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", \"comment\"]]\\\n",
    "        .astype({\"sourceCode\":\"string\", \"equivalence\":\"string\", \"conceptId\":\"Int64\"})\n",
    "\n",
    "# Value mappings\n",
    "df_val_wh = pd.read_excel(df_val_wh_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_val_sb = pd.read_excel(df_val_sb_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_val_cc = pd.read_excel(df_val_cc_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", 'comment']]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\", \"comment\":\"string\"})\n",
    "df_val_consensus = pd.read_excel(df_val_consensus_path)[[\"sourceCode\", \"equivalence\", \"conceptId\", \"comment\"]]\\\n",
    "        .astype({\"sourceCode\":\"Int64\", \"equivalence\":\"string\", \"conceptId\":\"Int64\"})\n",
    "\n",
    "assert df_el_consensus.shape == df_el_sb.shape == df_el_cc.shape == df_el_wh.shape\n",
    "assert df_val_consensus.shape == df_val_sb.shape == df_val_cc.shape == df_val_wh.shape\n",
    "\n",
    "apply_filter = True\n",
    "\n",
    "if apply_filter:\n",
    "    (df_el_wh, df_el_sb, df_el_cc, df_el_consensus) = custom_filter((df_el_wh, df_el_sb, df_el_cc, df_el_consensus), 'element')\n",
    "    (df_val_wh, df_val_sb, df_val_cc, df_val_consensus) = custom_filter((df_val_wh, df_val_sb, df_val_cc, df_val_consensus), 'value')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare mapping sheet sizes (SB vs CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_sb = (df_el_sb.shape[0], df_val_sb.shape[0])\n",
    "print(\"--Number of rows, Sally--\\n\\tElement Map: %d\\n\\tValue map: %d\" % (n_rows_sb[0], n_rows_sb[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_cc = (df_el_cc.shape[0], df_val_cc.shape[0])\n",
    "print(\"--Number of rows, Sally--\\n\\tElement Map: %d\\n\\tValue map: %d\" % (n_rows_cc[0], n_rows_cc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--Number of rows is equivalent--\\n\\tElement Map: %s\\n\\tValue map: %s\" % \\\n",
    "    (n_rows_cc[0] == n_rows_sb[0], n_rows_sb[1] == n_rows_cc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of instances where the conceptId is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "\n",
    "sr_eldiff = (df_el_sb.conceptId == df_el_cc.conceptId).value_counts()\n",
    "sr_valdiff = (df_val_sb.conceptId == df_val_cc.conceptId).value_counts()\n",
    "mat_values = {\"Elements\":sr_eldiff, \"Values\":sr_valdiff}\n",
    "\n",
    "df_compare_conceptid = pd.DataFrame(mat_values).rename({True:\"Matched\", False:\"Unmatched\"})\n",
    "df_compare_conceptid[\"Both\"] = df_compare_conceptid.sum(axis=1).astype(\"int\")\n",
    "df_compare_conceptid[\"% Element Agreement\"] = (df_compare_conceptid.Elements / df_compare_conceptid.Elements.sum())\n",
    "df_compare_conceptid[\"% Value Agreement\"] = (df_compare_conceptid.Values / df_compare_conceptid.Values.sum())\n",
    "df_compare_conceptid[\"% Overall Agreement\"] = (df_compare_conceptid.Both / df_compare_conceptid.Both.sum())\n",
    "\n",
    "df_compare_conceptid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of instances where both conceptId and equivalence are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "\n",
    "sr_eldiff = (df_el_sb.drop(columns=\"comment\") == df_el_cc.drop(columns=\"comment\")).all(axis=1).value_counts()\n",
    "sr_valdiff = (df_val_sb.drop(columns=\"comment\") == df_val_cc.drop(columns=\"comment\")).all(axis=1).value_counts()\n",
    "mat_values = {\"Elements\":sr_eldiff, \"Values\":sr_valdiff}\n",
    "\n",
    "df_compare_concequiv = pd.DataFrame(mat_values).rename({True:\"Matched\", False:\"Unmatched\"})\n",
    "df_compare_concequiv[\"Both\"] = df_compare_concequiv.sum(axis=1)\n",
    "df_compare_concequiv[\"% Element Agreement\"] = (df_compare_concequiv.Elements / df_compare_concequiv.Elements.sum())\n",
    "df_compare_concequiv[\"% Value Agreement\"] = (df_compare_concequiv.Values / df_compare_concequiv.Values.sum())\n",
    "df_compare_concequiv[\"% Overall Agreement\"] = (df_compare_concequiv.Both / df_compare_concequiv.Both.sum())\n",
    "\n",
    "df_compare_concequiv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of instances where conceptId matched, for elements excluding \"unmatched\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "\n",
    "ind_el_mappable = (df_el_sb.equivalence != \"UNMATCHED\") & (df_el_cc.equivalence != \"UNMATCHED\")\n",
    "ind_val_mappable = (df_val_sb.equivalence != \"UNMATCHED\") & (df_val_cc.equivalence != \"UNMATCHED\")\n",
    "\n",
    "df_el_sb_mappable = df_el_sb.loc[ind_el_mappable]\n",
    "df_el_cc_mappable = df_el_cc.loc[ind_el_mappable]\n",
    "df_val_sb_mappable = df_val_sb.loc[ind_val_mappable]\n",
    "df_val_cc_mappable = df_val_cc.loc[ind_val_mappable]\n",
    "\n",
    "verify_sourceCode_aligned(df_el_sb_mappable, df_el_cc_mappable)\n",
    "verify_sourceCode_aligned(df_val_sb_mappable, df_val_cc_mappable)\n",
    "\n",
    "sr_eldiff = (df_el_sb_mappable.conceptId == df_el_cc_mappable.conceptId).value_counts()\n",
    "sr_valdiff = (df_val_sb_mappable.conceptId == df_val_cc_mappable.conceptId).value_counts()\n",
    "mat_values = {\"Elements\":sr_eldiff, \"Values\":sr_valdiff}\n",
    "\n",
    "df_compare_mappable = pd.DataFrame(mat_values).rename({True:\"Matched\", False:\"Unmatched\"})\n",
    "df_compare_mappable[\"Both\"] = df_compare_mappable.sum(axis=1).astype(\"int\")\n",
    "df_compare_mappable[\"% Element Agreement\"] = (df_compare_mappable.Elements / df_compare_mappable.Elements.sum())\n",
    "df_compare_mappable[\"% Value Agreement\"] = (df_compare_mappable.Values / df_compare_mappable.Values.sum())\n",
    "df_compare_mappable[\"% Overall Agreement\"] = (df_compare_mappable.Both / df_compare_mappable.Both.sum())\n",
    "\n",
    "df_compare_mappable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare_conceptid.to_excel(outdir + \"/MapCompare/conceptid_percent_agreement.xlsx\", index=True)\n",
    "df_compare_concequiv.to_excel(outdir + \"/MapCompare/conceptid_and_equivalence_percent_agreement.xlsx\", index=True)\n",
    "df_compare_mappable.to_excel(outdir + \"/MapCompare/conceptid_percent_agreement_mappable_only.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Cohen's kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Equivalence label mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ismapped = {\"UNMATCHED\":1, \"NARROWER\":2, \"WIDER\":3, \"EQUAL\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_el_sb_ismapped = df_el_sb.equivalence.map(dict_ismapped)\n",
    "sr_el_cc_ismapped = df_el_cc.equivalence.map(dict_ismapped)\n",
    "sr_val_sb_ismapped = df_val_sb.equivalence.map(dict_ismapped)\n",
    "sr_val_cc_ismapped = df_val_cc.equivalence.map(dict_ismapped)\n",
    "\n",
    "el_kap = sklearn.metrics.cohen_kappa_score(sr_el_sb_ismapped, sr_el_cc_ismapped)\n",
    "val_kap = sklearn.metrics.cohen_kappa_score(sr_val_sb_ismapped, sr_val_cc_ismapped)\n",
    "total_kap = sklearn.metrics.cohen_kappa_score(pd.concat([sr_el_sb_ismapped, sr_val_sb_ismapped]),\\\n",
    "        pd.concat([sr_el_cc_ismapped, sr_val_cc_ismapped]))\n",
    "\n",
    "print(\"Element kappa: %.3f\" % el_kap)\n",
    "print(\"Value kappa: %.3f\" % val_kap)\n",
    "print(\"Total kappa: %.3f\" % total_kap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kappa = pd.DataFrame([[\"Elements\", el_kap], [\"Values\", val_kap], [\"Overall\", total_kap]], columns=[\"Subset\", \"Equiv Kappa\"])\n",
    "df_kappa.to_excel(outdir + \"/MapCompare/equivalence_kappa.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Whether it's mapped as mappable or unmappable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ismapped = {\"UNMATCHED\":False, \"NARROWER\":False, \"WIDER\":True, \"EQUAL\":True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_el_sb_ismapped = df_el_sb.equivalence.map(dict_ismapped)\n",
    "sr_el_cc_ismapped = df_el_cc.equivalence.map(dict_ismapped)\n",
    "sr_val_sb_ismapped = df_val_sb.equivalence.map(dict_ismapped)\n",
    "sr_val_cc_ismapped = df_val_cc.equivalence.map(dict_ismapped)\n",
    "\n",
    "el_kap = sklearn.metrics.cohen_kappa_score(sr_el_sb_ismapped, sr_el_cc_ismapped)\n",
    "val_kap = sklearn.metrics.cohen_kappa_score(sr_val_sb_ismapped, sr_val_cc_ismapped)\n",
    "total_kap = sklearn.metrics.cohen_kappa_score(pd.concat([sr_el_sb_ismapped, sr_val_sb_ismapped]),\\\n",
    "        pd.concat([sr_el_cc_ismapped, sr_val_cc_ismapped]))\n",
    "\n",
    "print(\"Element kappa: %.3f\" % el_kap)\n",
    "print(\"Value kappa: %.3f\" % val_kap)\n",
    "print(\"Total kappa: %.3f\" % total_kap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kappa = pd.DataFrame([[\"Elements\", el_kap], [\"Values\", val_kap], [\"Overall\", total_kap]], columns=[\"Subset\", \"Mapped/Unmapped Kappa\"])\n",
    "df_kappa.to_excel(outdir + \"/MapCompare/mappable_kappa.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "index_eldiff = ~(df_el_sb.drop(columns=\"comment\") == df_el_cc.drop(columns=\"comment\")).all(axis=1)\n",
    "index_valdiff = ~(df_val_sb.drop(columns=\"comment\") == df_val_cc.drop(columns=\"comment\")).all(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportfile = True\n",
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "\n",
    "temp_index_eldiff = ~(df_el_wh.drop(columns=\"comment\") == df_el_cc.drop(columns=\"comment\")).all(axis=1)\n",
    "\n",
    "df_elcompare = append_concept_names(df_el_wh)\\\n",
    "    .join(append_concept_names(df_el_cc).drop(columns=\"sourceCode\"), lsuffix=\"_sb\", rsuffix=\"_cc\")\\\n",
    "        .merge(df_eldef, left_on=\"sourceCode\", right_on=\"CUI\")\\\n",
    "            [[\"examArea\", \"dataElement\", \"sourceCode\", \"equivalence_sb\", \"conceptId_sb\", \"concept_name_sb\", \"equivalence_cc\", \"conceptId_cc\", \"concept_name_cc\"]]\\\n",
    "                .loc[temp_index_eldiff]\n",
    "\n",
    "if exportfile:\n",
    "    df_elcompare.to_excel(outdir + \"/MapCompare/CC_CONS_ElementDiscrepancies.xlsx\", index=False)\n",
    "    \n",
    "df_elcompare.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportfile = True\n",
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "\n",
    "df_elcompare = append_concept_names(df_el_sb)\\\n",
    "    .join(append_concept_names(df_el_cc).drop(columns=\"sourceCode\"), lsuffix=\"_sb\", rsuffix=\"_cc\")\\\n",
    "        .merge(df_eldef, left_on=\"sourceCode\", right_on=\"CUI\")\\\n",
    "            [[\"examArea\", \"dataElement\", \"sourceCode\", \"equivalence_sb\", \"conceptId_sb\", \"concept_name_sb\", \"equivalence_cc\", \"conceptId_cc\", \"concept_name_cc\"]]\\\n",
    "                .loc[index_eldiff]\n",
    "\n",
    "if exportfile:\n",
    "    df_elcompare.to_excel(outdir + \"/MapCompare/ElementDiscrepancies.xlsx\", index=False)\n",
    "    \n",
    "df_elcompare.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportfile = True\n",
    "verify_sourceCode_aligned(df_el_sb, df_el_cc)     # Should do this every time\n",
    "verify_sourceCode_aligned(df_val_sb, df_val_cc)     # Should do this every time\n",
    "\n",
    "df_valcompare = append_concept_names(df_val_sb)\\\n",
    "    .join(append_concept_names(df_val_cc).drop(columns=\"sourceCode\"), lsuffix=\"_sb\", rsuffix=\"_cc\")\\\n",
    "        .merge(df_valdef, left_on=\"sourceCode\", right_on=\"ID\")\\\n",
    "            .merge(df_eldef, left_on=\"CUI\", right_on=\"CUI\")\\\n",
    "                [[\"examArea\", \"dataElement\", \"value\", \"sourceCode\", \"equivalence_sb\", \"conceptId_sb\", \"concept_name_sb\", \"equivalence_cc\", \"conceptId_cc\", \"concept_name_cc\"]]\\\n",
    "                    .loc[index_valdiff]\n",
    "\n",
    "if exportfile:\n",
    "    df_valcompare.to_excel(outdir + \"/MapCompare/ValueDiscrepancies.xlsx\", index=False)\n",
    "\n",
    "df_valcompare.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DataFrame to hold our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_list = [\"wh\", \"sb\", \"cc\", \"cons\"]\n",
    "lab_dict = {\"wh\":[df_el_wh, df_val_wh], \"sb\":[df_el_sb, df_val_sb], \"cc\":[df_el_cc, df_val_cc], \"cons\":[df_el_consensus, df_val_consensus]}\n",
    "iterables = [lab_list, [\"conceptId\", \"equivalence\", \"both\", \"sum\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"grader\", \"diff_source\"])\n",
    "m_columns = pd.MultiIndex.from_product([lab_list, [\"#\", '%']], names=[\"grader\", \"diff_source\"])\n",
    "df_rowchange = pd.DataFrame(columns=m_columns, index=index).astype(\"Int64\")\\\n",
    "    .astype({(\"wh\",\"%\"):\"float\", (\"sb\",\"%\"):\"float\", (\"cc\",\"%\"):\"float\", (\"cons\",\"%\"):\"float\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through to append the values into this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_els = df_el_consensus.shape[0] + df_val_consensus.shape[0]\n",
    "\n",
    "for col_lab in lab_list:\n",
    "    for row_lab in lab_list:\n",
    "        # Load the dataframes to compare\n",
    "        df_col_el = lab_dict[col_lab][0].drop(columns=\"comment\").fillna(0)\n",
    "        df_col_val = lab_dict[col_lab][1].drop(columns=\"comment\").fillna(0)\n",
    "        df_row_el = lab_dict[row_lab][0].drop(columns=\"comment\").fillna(0)\n",
    "        df_row_val = lab_dict[row_lab][1].drop(columns=\"comment\").fillna(0)\n",
    "\n",
    "        verify_sourceCode_aligned(df_col_el, df_row_el)\n",
    "        verify_sourceCode_aligned(df_col_val, df_row_val)\n",
    "        \n",
    "        conc_diff = ((df_col_el.equivalence == df_row_el.equivalence) & ~(df_col_el.conceptId == df_row_el.conceptId)).sum()\\\n",
    "            + ((df_col_val.equivalence == df_row_val.equivalence) & ~(df_col_val.conceptId == df_row_val.conceptId)).sum()\n",
    "        eq_diff = (~(df_col_el.equivalence == df_row_el.equivalence) & (df_col_el.conceptId == df_row_el.conceptId)).sum()\\\n",
    "            + (~(df_col_val.equivalence == df_row_val.equivalence) & (df_col_val.conceptId == df_row_val.conceptId)).sum()\n",
    "        both_diff = (~(df_col_el.equivalence == df_row_el.equivalence) & ~(df_col_el.conceptId == df_row_el.conceptId)).sum()\\\n",
    "            + (~(df_col_val.equivalence == df_row_val.equivalence) & ~(df_col_val.conceptId == df_row_val.conceptId)).sum()\n",
    "        sum_diff = (~((df_col_el == df_row_el).all(axis=1))).sum()\\\n",
    "            + (~((df_col_val == df_row_val).all(axis=1))).sum()\n",
    "\n",
    "        assert sum_diff == eq_diff + conc_diff + both_diff\n",
    "        assert df_col_el.shape[0] + df_col_val.shape[0] == n_els\n",
    "        assert df_row_el.shape[0] + df_row_val.shape[0] == n_els\n",
    "\n",
    "        df_rowchange.loc[(row_lab, \"conceptId\"), (col_lab, \"#\")] = conc_diff\n",
    "        df_rowchange.loc[(row_lab, \"equivalence\"), (col_lab, \"#\")] = eq_diff\n",
    "        df_rowchange.loc[(row_lab, \"both\"), (col_lab, \"#\")] = both_diff\n",
    "        df_rowchange.loc[(row_lab, \"sum\"), (col_lab, \"#\")] = sum_diff\n",
    "        df_rowchange.loc[(row_lab, \"conceptId\"), (col_lab, \"%\")] = conc_diff / n_els\n",
    "        df_rowchange.loc[(row_lab, \"equivalence\"), (col_lab, \"%\")] = eq_diff / n_els\n",
    "        df_rowchange.loc[(row_lab, \"both\"), (col_lab, \"%\")] = both_diff / n_els\n",
    "        df_rowchange.loc[(row_lab, \"sum\"), (col_lab, \"%\")] = sum_diff / n_els"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rowchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rowchange.to_excel(outdir + \"/MapCompare/rowchange.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ucsdVocabMap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8911727b8d453bd647756eb3276a698202ff1bf11ce1bf5892797b533680b196"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
